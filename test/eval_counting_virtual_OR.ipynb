{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly Supervised Instance Segmentation using Class Peak Response \n",
    "### Evaluation code of object counting in COCO\n",
    "\n",
    "Published Results: mRMSE 0.29, mRMSE-nz 1.14, m-relRMSE 0.17, m-relRMSE-nz 0.61 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from nest import modules, run_tasks\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "## make sure you have correctly install cocoapi, https://github.com/cocodataset/cocoapi\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import pycocotools.mask as mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "def mrmse(non_zero, count_pred, count_gt): \n",
    "    nzero_mask=torch.ones(count_gt.size()) \n",
    "    if non_zero==1:\n",
    "        nzero_mask=torch.zeros(count_gt.size())\n",
    "        nzero_mask[count_gt!=0]=1 \n",
    "    mrmse = torch.pow(count_pred - count_gt, 2) \n",
    "    mrmse = torch.mul(mrmse, nzero_mask) \n",
    "    mrmse = torch.sum(mrmse, 0) \n",
    "    nzero = torch.sum(nzero_mask, 0) \n",
    "    mrmse = torch.div(mrmse, nzero) \n",
    "    # Alteration to set nan values to 0 - J.I. \n",
    "    mrmse[torch.isnan(mrmse)] = 0 \n",
    "    mrmse = torch.sqrt(mrmse)  \n",
    "    mrmse = torch.mean(mrmse) \n",
    "    return mrmse\n",
    "\n",
    "def rel_mrmse(non_zero,count_pred, count_gt):\n",
    "    nzero_mask=torch.ones(count_gt.size())\n",
    "    if non_zero==1:\n",
    "        nzero_mask=torch.zeros(count_gt.size())\n",
    "        nzero_mask[count_gt!=0]=1\n",
    "    num = torch.pow(count_pred - count_gt, 2)\n",
    "    denom = count_gt.clone()\n",
    "    denom = denom+1\n",
    "    rel_mrmse = torch.div(num, denom)\n",
    "    rel_mrmse = torch.mul(rel_mrmse, nzero_mask)\n",
    "    rel_mrmse = torch.sum(rel_mrmse, 0)\n",
    "    nzero = torch.sum(nzero_mask, 0)\n",
    "    rel_mrmse = torch.div(rel_mrmse, nzero)\n",
    "    # Alteration to set nan values to 0 - J.I. \n",
    "    rel_mrmse[torch.isnan(rel_mrmse)] = 0 \n",
    "    rel_mrmse = torch.sqrt(rel_mrmse)\n",
    "    rel_mrmse = torch.mean(rel_mrmse)\n",
    "    return rel_mrmse\n",
    "\n",
    "# image pre-processor\n",
    "image_size = 448\n",
    "transformer = modules.image_transform(\n",
    "    image_size = [image_size, image_size],\n",
    "    augmentation = dict(),\n",
    "    mean = [0.485, 0.456, 0.406],\n",
    "    std = [0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "## load ground truth\n",
    "# coco_path='../../../../../../../media/James/BigData/COCO/'\n",
    "coco_path='../../../../../../../media/James/BigData/Virtual_OR_Dataset'\n",
    "#'/raid/xiaoke/MSCOCO/coco'\n",
    " \n",
    "# cocoGt=COCO(coco_path+'/annotations/virtual_OR_dataset_annotation.json')\n",
    "cocoGt=COCO(coco_path+'/annotations/virtual_OR_test.json')\n",
    "image_ids=cocoGt.getImgIds()\n",
    "\n",
    "catids=cocoGt.getCatIds()\n",
    "\n",
    "num_classes=len(catids)\n",
    "catid2index={}\n",
    "for i,cid in enumerate(catids):\n",
    "    catid2index[cid]=i\n",
    "annids=cocoGt.getAnnIds()\n",
    "class_labels = OrderedDict()\n",
    "for id in annids:\n",
    "    anns=cocoGt.loadAnns(id)\n",
    "    for i in range(len(anns)):\n",
    "        ann=anns[i]\n",
    "        name=ann['image_id']\n",
    "        if name not in class_labels:\n",
    "            class_labels[name]=np.zeros(num_classes)\n",
    "        category_id=ann['category_id']\n",
    "        class_labels[name][catid2index[category_id]]+=1\n",
    "\n",
    "image_list_all=list(class_labels.keys())\n",
    "gt_counts_all=[gt[1] for gt in list(class_labels.items())]\n",
    "gt_counts_all=np.array(gt_counts_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_list_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b15690e4601c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;31m#int(40137/2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimage_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_list_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgt_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt_counts_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_list_all' is not defined"
     ]
    }
   ],
   "source": [
    "## use the first half as validation and second half as test\n",
    "index=5#int(40137/2)\n",
    "print(index)\n",
    "image_list=image_list_all[index:]\n",
    "gt_count=gt_counts_all[index:]\n",
    "\n",
    "## load model\n",
    "backbone = modules.fc_resnet50(channels=30, pretrained=False)\n",
    "model = modules.peak_response_mapping(backbone,enable_peak_stimulation=True,peak_stimulation='addedmodule5',\n",
    "                                     sub_pixel_locating_factor=1)\n",
    "model = nn.DataParallel(model)\n",
    "checkpoint = torch.load('../../../../../../../media/James/BigData/CountSeg_output/vOR_counting/model_latest.pt')#('../models/counting/coco14.pt') \n",
    "model_dict = model.state_dict()\n",
    "trained_dict = {k: v for k, v in checkpoint['model'].items() if k in model_dict}\n",
    "model_dict.update(trained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "model = model.module.cuda()\n",
    "#print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the new implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "mRMSE tensor(0.6603)\n",
      "mRMSE_nz tensor(1.4955)\n",
      "rel_mRMSE tensor(0.2480)\n",
      "rel_mRMSE_nz tensor(0.5627)\n"
     ]
    }
   ],
   "source": [
    "## counting\n",
    "pred_class=[]\n",
    "pred_count=[]\n",
    "\n",
    "for index_d,ima in enumerate(image_list):\n",
    "    if index_d%10000==0:\n",
    "        print(index_d)\n",
    "    raw_img = Image.open(coco_path+'/images/'+'0'*(12-len(str(ima)))+str(ima)+'.png').convert('RGB')\n",
    "    width, height=raw_img.size \n",
    "    input_var = transformer(raw_img).unsqueeze(0).cuda().requires_grad_()\n",
    "    model = model.eval()\n",
    "    confidence,class_response_map1,peak = model(input_var,1)\n",
    "    confidence=confidence.cpu().detach().numpy()\n",
    "    count_one = F.adaptive_avg_pool2d(class_response_map1, 1).squeeze(2).squeeze(2).detach().cpu().numpy()[0]\n",
    "    confidence[confidence<0]=0\n",
    "    confidence=confidence[0]\n",
    "    confidence[confidence>0]=1\n",
    "    pred_class.append(confidence)\n",
    "    pred_count.append(np.round(confidence*count_one))\n",
    "pred_count=np.array(pred_count)\n",
    "\n",
    "print('mRMSE', mrmse(0,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n",
    "print('mRMSE_nz', mrmse(1,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n",
    "print('rel_mRMSE', rel_mrmse(0,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))\n",
    "print('rel_mRMSE_nz', rel_mrmse(1,torch.from_numpy(pred_count).float(), torch.from_numpy(gt_count).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parsed_json = (json.loads(json_data))\n",
    "\n",
    "# with open(coco_path+'/annotations/virtual_OR_dataset_annotation(v2).json', 'r') as f:\n",
    "#     distros_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distros_dict['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_d,ima in enumerate(image_list):\n",
    "    raw_img = Image.open(coco_path+'/images/'+'0'*(12-len(str(ima)))+str(ima)+'.png').convert('RGB')\n",
    "    width, height=raw_img.size \n",
    "    input_var = transformer(raw_img).unsqueeze(0).cuda().requires_grad_()\n",
    "    model = model.eval()\n",
    "    confidence,class_response_map1,peak = model(input_var,1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class_ID = 0\n",
    "IMG = class_response_map1[0][class_ID].cpu().detach().numpy()\n",
    "# Image.fromarray(IMG )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG = IMG + 1\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
